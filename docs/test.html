<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>test API documentation</title>
<meta name="description" content="Easy usage of Netket + my custom code â€¦" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>test</code></h1>
</header>
<section id="section-intro">
<p>Easy usage of Netket + my custom code</p>
<p>This script allows the user to train a machine using the run function. A machine can also be loaded with the load function.
To measure an observable, you can use the function measureObservable.
Furthermore, exact results can be computed with the method exact.
It is recommended to use few samples plus many iterations with the run method and many samples plus few iterations with the load method.
The supported machines are defined in my_machines.py, the supported hamiltonians are defined in my_models.py, and the observables are defined in my_operators.py.
To use this file, you have to import it and use its functions.</p>
<p>This project requires the following libraries:
netket, numpy, scipy, jax, jaxlib, networkx, torch, tqdm, matplotlib</p>
<p>This file contains the following functions:</p>
<pre><code>* run
* load
* measureObservables
* exact
</code></pre>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Easy usage of Netket + my custom code

This script allows the user to train a machine using the run function. A machine can also be loaded with the load function.
To measure an observable, you can use the function measureObservable.
Furthermore, exact results can be computed with the method exact.
It is recommended to use few samples plus many iterations with the run method and many samples plus few iterations with the load method.
The supported machines are defined in my_machines.py, the supported hamiltonians are defined in my_models.py, and the observables are defined in my_operators.py.
To use this file, you have to import it and use its functions.

This project requires the following libraries:
netket, numpy, scipy, jax, jaxlib, networkx, torch, tqdm, matplotlib

This file contains the following functions:

    * run
    * load
    * measureObservables
    * exact
&#34;&#34;&#34;
import netket as nk
import my_models as models
import my_machines as machines
import my_operators as operators
import helping_functions as functions

import time
import numpy as np
import scipy as sp
import sys
import jax
import csv






__L__ = 50
__number_samples__ = 700
__number_iterations__ = 500
__alpha__ = 4

#use Gd, if Sr == None; otherwise, sr is the diag_shift
def run(L=__L__, alpha=__alpha__, sr = None, dataname = None, path = &#39;run&#39;, machine_name = &#39;JaxRBM&#39;, sampler = &#39;Local&#39;, hamiltonian_name = &#39;transformed_Heisenberg&#39;, n_samples = __number_samples__, n_iterations = __number_iterations__):
    &#34;&#34;&#34;Method to train a machine.

        A hamiltonian and sampler can be chosen. The machine is defined and trained for the hamiltonian.

            Args:
                L (int) : The number of sites of the lattice.
                alpha (int) : A factor to define the size of different machines.
                sr (float) : The parameter for stochastic reconfiguration method. If it is None, stochastic reconfiguration is not used.
                dataname (str) : The dataname. If None, an automatic dataname is chosen
                path (str) : The directory, where the results are saved. If None, the directory is &#39;run&#39;
                machine_name (str) A string to choose the machine. Possible inputs: See get_machine in my_machines.py
                sampler (str) : A string to choose the sampler: Recommended: &#39;Local&#39; (this works with every machine)
                hamiltonian_name (str) : A string to choose the hamiltonian. Possible inputs: see get_hamiltonian in my_models.py
                n_samples (int) : The number of samples used in every iteration step
                n_iterations (int) : The number of iterations (training steps)
                                                    &#34;&#34;&#34;
    ha, hi, g = models.get_hamiltonian(hamiltonian_name, L)
    print(&#39;uses&#39;, hamiltonian_name, &#39;hamiltonian&#39;)
    sys.stdout.flush()
    generate_machine = machines.get_machine(machine_name)
    ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha, optimizer=&#39;Adamax&#39;, lr=0.005, sampler=sampler)

    if(sr == None):
        gs = nk.Vmc(hamiltonian=ha, sampler=sa, optimizer=op, n_samples=n_samples)
    else:
        sr = nk.optimizer.SR(ma, diag_shift=sr)
        gs = nk.Vmc(hamiltonian=ha, sampler=sa, optimizer=op, n_samples=n_samples, sr=sr)

    #observables = functions.get_operator(hilbert=hi, L=L, operator=&#39;FerroCorr&#39;)
    if(dataname == None):
        dataname = &#39;&#39;.join((&#39;L&#39;, str(L)))
    dataname = functions.create_path(dataname, path=path)
    print(&#39;&#39;)
    functions.create_machinefile(machine_name, L, alpha, dataname, sr)
    start = time.time()
    gs.run(n_iter=int(n_iterations), out=dataname)#, obs=observables)
    end = time.time()
    with open(&#39;&#39;.join((dataname, &#39;.time&#39;)), &#39;w&#39;) as reader:
        reader.write(str(end - start))
    print(&#39;Time&#39;, end - start)
    sys.stdout.flush()


#ensure, that the machine is the same as used before!
def load(L=__L__, alpha=__alpha__, sr = None, dataname = None, path = &#39;run&#39;, machine_name = &#39;JaxRBM&#39;, sampler = &#39;Local&#39;, hamiltonian_name = &#39;transformed_Heisenberg&#39;, n_samples =10000, n_iterations = 20):
    &#34;&#34;&#34;Method to load a pretrained machine and continue the training.

        A hamiltonian and sampler can be chosen. The machine is defined and trained for the hamiltonian.

            Args:
                L (int) : The number of sites of the lattice
                alpha (int) : A factor to define the size of different machines
                sr (float) : The parameter for stochastic reconfiguration method. If it is None, stochastic reconfiguration is not used
                dataname (str) : The dataname. If None, an automatic dataname is chosen
                path (str) : The directory, where the results are saved. If None, the directory is &#39;run&#39;
                machine_name (str) A string to choose the machine. Possible inputs: See get_machine in my_machines.py
                sampler (str) : A string to choose the sampler: Recommended: &#39;Local&#39; (this works with every machine)
                hamiltonian_name (str) : A string to choose the hamiltonian. Possible inputs: see get_hamiltonian in my_models.py
                n_samples (int) : The number of samples used in every iteration step
                n_iterations (int) : The number of iterations (training steps)

                &#34;&#34;&#34;
    if (dataname == None):
        dataname = &#39;&#39;.join((&#39;L&#39;, str(L)))
    dataname = functions.create_path(dataname, path=path)
    ha, hi, g = models.get_hamiltonian(hamiltonian_name, L)
    print(&#39;uses&#39;, hamiltonian_name, &#39;hamiltonian&#39;)
    sys.stdout.flush()
    print(&#39;load the machine: &#39;, dataname)
    generate_machine = machines.get_machine(machine_name)
    ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
    ma.load(&#39;&#39;.join((dataname, &#39;.wf&#39;)))
    op, sa = machines.load_machine(machine=ma, hamiltonian=ha, optimizer=&#39;Adamax&#39;, lr=0.001, sampler=sampler)
    #observables = functions.get_operator(hilbert=hi, L=L, operator=&#39;FerroCorr&#39;, symmetric=True)
    #observables = {**functions.get_operator(hilbert=hi, L=L, operator=&#39;FerroCorr&#39;, symmetric=False), **functions.get_operator(hilbert=hi, L=L, operator=&#39;FerroCorr&#39;, symmetric=True)}
    if(sr == None):
        gs2 = nk.Vmc(hamiltonian=ha, sampler=sa, optimizer=op, n_samples=n_samples)#, n_discard=5000)
    else:
        sr = nk.optimizer.SR(ma, diag_shift=sr)
        gs2 = nk.Vmc(hamiltonian=ha, sampler=sa, optimizer=op, n_samples=n_samples, sr=sr)#, n_discard=5000)

    functions.create_machinefile(machine_name, L, alpha, dataname, sr)
    start = time.time()
    #gs2.run(n_iter=n_iterations, out=&#39;&#39;.join((dataname, &#39;_load&#39;)), obs=observables, write_every=4, save_params_every=4)
    gs2.run(n_iter=n_iterations, out=dataname, write_every=10, save_params_every=10)
    end = time.time()
    with open(&#39;&#39;.join((dataname, &#39;.time&#39;)), &#39;a&#39;) as reader:
        reader.write(str(end - start))
    #print(gs2.estimate(observables))
    print(&#39;Time&#39;, end - start)
    sys.stdout.flush()


def measureObservable(L=__L__, alpha=__alpha__, dataname = None, path = &#39;run&#39;, machine_name = &#39;JaxRBM&#39;, sampler = &#39;Local&#39;, hamiltonian_name = &#39;transformed_Heisenberg&#39;, n_samples =10000, n_iterations = 20):
    &#34;&#34;&#34;Method to measure th observables with a trained machine.

            The sampler can be chosen.

                Args:
                    L (int) : The number of sites of the lattice
                    alpha (int) : A factor to define the size of different machines
                    dataname (str) : The dataname. If None, an automatic dataname is chosen
                    path (str) : The directory, where the results are saved. If None, the directory is &#39;run&#39;
                    machine_name (str) A string to choose the machine. Possible inputs: See get_machine in my_machines.py
                    sampler (str) : A string to choose the sampler: Recommended: &#39;Local&#39; (this works with every machine)
                    hamiltonian_name (str) : A string to choose the hamiltonian. Possible inputs: see get_hamiltonian in my_models.py
                    n_samples (int) : The number of samples used in every iteration step
                    n_iterations (int) : The number of iterations (training steps)

                    &#34;&#34;&#34;
    if (dataname == None):
        dataname = &#39;&#39;.join((&#39;L&#39;, str(L)))
    dataname = functions.create_path(dataname, path=path)
    ha, hi, g = models.get_hamiltonian(hamiltonian_name, L)
    generate_machine = machines.get_machine(machine_name)
    ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
    ma.load(&#39;&#39;.join((dataname, &#39;.wf&#39;)))
    op, sa = machines.load_machine(machine=ma, hamiltonian=ha, optimizer=&#39;Adamax&#39;, lr=0.001, sampler=sampler)
    observables = {**functions.get_operator(hilbert=hi, L=L, operator=&#39;FerroCorr&#39;, symmetric=False),
                   **functions.get_operator(hilbert=hi, L=L, operator=&#39;FerroCorr&#39;, symmetric=True)}
    #save_dict = {}
    for i in range(n_iterations):
        measurement = nk.variational.estimate_expectations(observables, sa, n_samples=n_samples)
        #save_dict[&#39;&#39;.join((&#39;Iteration&#39;, str(i)))] = measurement
        if(i == 0):
            w = csv.writer(open(&#39;&#39;.join((dataname, &#39;_observables&#39;, &#39;.csv&#39;)), &#34;w&#34;))
            for key, val in measurement.items():
                w.writerow([key, val])
        else:
            w = csv.writer(open(&#39;&#39;.join((dataname, &#39;_observables&#39;, &#39;.csv&#39;)), &#34;a&#34;))
            for key, val in measurement.items():
                w.writerow([key, val])
        print(measurement)



def exact(L = __L__, symmetric = True, dataname = None, path = &#39;run&#39;, hamiltonian_name = &#39;transformed_Heisenberg&#39;):
    &#34;&#34;&#34;Method to solve a hamiltonian exactly.

        A hamiltonian can be chosen. The energy is evaluated using the lanczos method
        and a observable is evaluated with the power method.
        Use this only for small lattices, because it needs an awful lot of RAM.

            Args:
                L (int): The number of sites of the lattice
                symmetric (bool) :
                    If True, the evaluated observable is symmetric to the center of the lattice.
                    If false, it starts at one end of the lattice.
                dataname (str) : The dataname. If None, an automatic dataname is chosen
                path (str) : The directory, where the results are saved. If None, the directory is &#39;run&#39;
                hamiltonian_name (str) : A string to choose the hamiltonian. Possible inputs: &#39;transformed_Heisenberg&#39;, &#39;original_Heisenberg&#39;

                &#34;&#34;&#34;
    ha, hi, g = models.get_hamiltonian(hamiltonian_name, L)
    print(&#39;uses&#39;, hamiltonian_name, &#39;hamiltonian&#39;)
    sys.stdout.flush()

    w, v_tmp = sp.sparse.linalg.eigsh(ha.to_sparse(), k=1, which=&#39;SR&#39;, return_eigenvectors=True)
    #w, v_tmp = sp.linalg.eigh(ha.to_dense())
    print(v_tmp.shape)
    print(&#39;Energy:&#39;, w[0], &#39;Lattice size:&#39;, L)
    sys.stdout.flush()
    # v = np.empty(3**L, dtype=np.complex128)
    # print(v.shape)
    # for index, i in enumerate(v_tmp[:, 0]):
    #     v[index] = i

    v = functions.power_method(ha.to_sparse(), L, w[0])

    if(symmetric == True):
        results = np.empty(int(L/2) -1 + L%2, dtype=np.float64)
        for index, i in enumerate(range(1, int(L / 2.) + L%2)):
            if (hamiltonian_name == &#39;transformed_Heisenberg&#39; or hamiltonian_name == &#39;transformed_AKLT&#39;):
                observable = operators.FerroCorrelationZ(hilbert=hi, j=int(L / 2.) - i, k=int(L / 2.) + i).to_sparse()
            else:
                print(&#39;Not implemented yet. Do not use the symmetric operator!&#39;)
                sys.stdout.flush()
                observable = None
            result_l = observable.dot(v).dot(v).real
            results[index] = result_l
    else:
        results = np.empty(L-1, dtype=np.float64)
        for index, i in enumerate(range(1, L)):
            if (hamiltonian_name == &#39;transformed_Heisenberg&#39; or hamiltonian_name == &#39;transformed_AKLT&#39;):
                observable = operators.FerroCorrelationZ(hilbert=hi, j=0, k=i).to_sparse()
            else:
                observable = operators.StringCorrelation(hilbert=hi, j=0, k=i).to_sparse()
            #print(observable.shape)
            #print(v.shape)
            #result_l = np.dot(np.dot(v, observable), v).real
            result_l = observable.dot(v).dot(v).real
            #print(result_l, &#39;; &#39;, result_l2)
            results[index] = result_l
    if(dataname == None):
        dataname = &#39;&#39;.join((&#39;L&#39;, str(L), &#39;_exact&#39;))
    dataname = functions.create_path(dataname, path=path)
    dataname = &#39;&#39;.join((dataname, &#39;.csv&#39;))
    # save to csv file
    np.savetxt(dataname, results, delimiter=&#39;;&#39;)
    print(results)
    sys.stdout.flush()
    return results


#run(L=5, alpha=10, sr=0.01, path=&#39;test_sr&#39;, dataname=&#39;test_sr&#39;, n_samples=300, n_iterations=50, machine_name=&#39;JaxFFNN&#39;)
#load(L=5, alpha=10, sr=0.01, path=&#39;test_sr&#39;, dataname=&#39;test_sr&#39;, n_samples=3000, n_iterations=20, machine_name=&#39;JaxFFNN&#39;)

#exact(L=10, symmetric=False, transformed=True)

#jax.config.update(&#39;jax_disable_jit&#39;, True)
#run(L=4, alpha=2, n_samples=300, n_iterations=300, machine_name=&#39;JaxFFNN&#39;, sampler=&#39;VBS&#39;)

#run(L=12, alpha=6, machine_name=&#39;JaxDeepFFNN&#39;, sampler=&#39;Local&#39;, hamiltonian_name=&#39;transformed_Heisenberg&#39;, n_samples=500, n_iterations=150)
#load(L=12, alpha=6, machine_name=&#39;JaxDeepFFNN&#39;, sampler=&#39;Local&#39;, hamiltonian_name=&#39;transformed_Heisenberg&#39;, n_samples=100, n_iterations=100)
#measureObservable(L=12, alpha=6, machine_name=&#39;JaxDeepFFNN&#39;, sampler=&#39;Local&#39;, hamiltonian_name=&#39;transformed_Heisenberg&#39;, n_samples=2000, n_iterations=100)



# L=16
# alpha0 = 100
# ha, hi, g = models.build_Heisenbergchain_S1_transformed(L=L)
#
# alpha = alpha0
# print(alpha)
# machine_name = &#39;JaxRBM&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# compare = ma.n_par
# print(machine_name, ma.n_par)
#
# alpha = 16*alpha0
# print(alpha)
# machine_name = &#39;JaxSymmRBM&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(0.94 * alpha0)
# print(alpha)
# machine_name = &#39;JaxFFNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(alpha0 * 0.1)
# print(alpha)
# machine_name = &#39;JaxDeepFFNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(0.14*alpha0)
# print(alpha)
# machine_name = &#39;JaxDeepConvNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(0.1*alpha0)
# print(alpha)
# machine_name = &#39;JaxSymmFFNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(alpha0 * 0.35)
# print(alpha)
# machine_name = &#39;JaxUnaryFFNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(alpha0 * 0.11)
# print(alpha)
# machine_name = &#39;JaxConv3NN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(alpha0 * 0.05)
# print(alpha)
# machine_name = &#39;JaxResFFNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(alpha0 * 0.33)
# print(alpha)
# machine_name = &#39;JaxResConvNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)




# L=30
# alpha0 = 100
# ha, hi, g = models.build_Heisenbergchain_S1_transformed(L=L)
#
# alpha = alpha0
# print(alpha)
# machine_name = &#39;JaxRBM&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# compare = ma.n_par
# print(machine_name, ma.n_par)
#
# alpha = 30*alpha0
# print(alpha)
# machine_name = &#39;JaxSymmRBM&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(0.94 * alpha0)
# print(alpha)
# machine_name = &#39;JaxFFNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(alpha0 * 0.1)
# print(alpha)
# machine_name = &#39;JaxDeepFFNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(0.12*alpha0)
# print(alpha)
# machine_name = &#39;JaxDeepConvNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(0.1*alpha0)
# print(alpha)
# machine_name = &#39;JaxSymmFFNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(alpha0 * 0.34)
# print(alpha)
# machine_name = &#39;JaxUnaryFFNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(alpha0 * 0.11)
# print(alpha)
# machine_name = &#39;JaxConv3NN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(alpha0 * 0.05)
# print(alpha)
# machine_name = &#39;JaxResFFNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(alpha0 * 0.61)
# print(alpha)
# machine_name = &#39;JaxResConvNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)



# L=50
# alpha0 = 100
# ha, hi, g = models.build_Heisenbergchain_S1_transformed(L=L)
#
# alpha = alpha0
# print(alpha)
# machine_name = &#39;JaxRBM&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# compare = ma.n_par
# print(machine_name, ma.n_par)
#
# alpha = 50*alpha0
# print(alpha)
# machine_name = &#39;JaxSymmRBM&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(0.98 * alpha0)
# print(alpha)
# machine_name = &#39;JaxFFNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(alpha0 * 0.1)
# print(alpha)
# machine_name = &#39;JaxDeepFFNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(0.11*alpha0)
# print(alpha)
# machine_name = &#39;JaxDeepConvNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(0.1*alpha0)
# print(alpha)
# machine_name = &#39;JaxSymmFFNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(alpha0 * 0.34)
# print(alpha)
# machine_name = &#39;JaxUnaryFFNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(alpha0 * 0.10)
# print(alpha)
# machine_name = &#39;JaxConv3NN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(alpha0 * 0.05)
# print(alpha)
# machine_name = &#39;JaxResFFNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)
#
# alpha = int(alpha0 * 1.02)
# print(alpha)
# machine_name = &#39;JaxResConvNN&#39;
# generate_machine = machines.get_machine(machine_name)
# ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
# print(machine_name, ma.n_par, compare/ma.n_par)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="test.exact"><code class="name flex">
<span>def <span class="ident">exact</span></span>(<span>L=50, symmetric=True, dataname=None, path='run', hamiltonian_name='transformed_Heisenberg')</span>
</code></dt>
<dd>
<div class="desc"><p>Method to solve a hamiltonian exactly.</p>
<p>A hamiltonian can be chosen. The energy is evaluated using the lanczos method
and a observable is evaluated with the power method.
Use this only for small lattices, because it needs an awful lot of RAM.</p>
<pre><code>Args:
    L (int): The number of sites of the lattice
    symmetric (bool) :
        If True, the evaluated observable is symmetric to the center of the lattice.
        If false, it starts at one end of the lattice.
    dataname (str) : The dataname. If None, an automatic dataname is chosen
    path (str) : The directory, where the results are saved. If None, the directory is 'run'
    hamiltonian_name (str) : A string to choose the hamiltonian. Possible inputs: 'transformed_Heisenberg', 'original_Heisenberg'
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def exact(L = __L__, symmetric = True, dataname = None, path = &#39;run&#39;, hamiltonian_name = &#39;transformed_Heisenberg&#39;):
    &#34;&#34;&#34;Method to solve a hamiltonian exactly.

        A hamiltonian can be chosen. The energy is evaluated using the lanczos method
        and a observable is evaluated with the power method.
        Use this only for small lattices, because it needs an awful lot of RAM.

            Args:
                L (int): The number of sites of the lattice
                symmetric (bool) :
                    If True, the evaluated observable is symmetric to the center of the lattice.
                    If false, it starts at one end of the lattice.
                dataname (str) : The dataname. If None, an automatic dataname is chosen
                path (str) : The directory, where the results are saved. If None, the directory is &#39;run&#39;
                hamiltonian_name (str) : A string to choose the hamiltonian. Possible inputs: &#39;transformed_Heisenberg&#39;, &#39;original_Heisenberg&#39;

                &#34;&#34;&#34;
    ha, hi, g = models.get_hamiltonian(hamiltonian_name, L)
    print(&#39;uses&#39;, hamiltonian_name, &#39;hamiltonian&#39;)
    sys.stdout.flush()

    w, v_tmp = sp.sparse.linalg.eigsh(ha.to_sparse(), k=1, which=&#39;SR&#39;, return_eigenvectors=True)
    #w, v_tmp = sp.linalg.eigh(ha.to_dense())
    print(v_tmp.shape)
    print(&#39;Energy:&#39;, w[0], &#39;Lattice size:&#39;, L)
    sys.stdout.flush()
    # v = np.empty(3**L, dtype=np.complex128)
    # print(v.shape)
    # for index, i in enumerate(v_tmp[:, 0]):
    #     v[index] = i

    v = functions.power_method(ha.to_sparse(), L, w[0])

    if(symmetric == True):
        results = np.empty(int(L/2) -1 + L%2, dtype=np.float64)
        for index, i in enumerate(range(1, int(L / 2.) + L%2)):
            if (hamiltonian_name == &#39;transformed_Heisenberg&#39; or hamiltonian_name == &#39;transformed_AKLT&#39;):
                observable = operators.FerroCorrelationZ(hilbert=hi, j=int(L / 2.) - i, k=int(L / 2.) + i).to_sparse()
            else:
                print(&#39;Not implemented yet. Do not use the symmetric operator!&#39;)
                sys.stdout.flush()
                observable = None
            result_l = observable.dot(v).dot(v).real
            results[index] = result_l
    else:
        results = np.empty(L-1, dtype=np.float64)
        for index, i in enumerate(range(1, L)):
            if (hamiltonian_name == &#39;transformed_Heisenberg&#39; or hamiltonian_name == &#39;transformed_AKLT&#39;):
                observable = operators.FerroCorrelationZ(hilbert=hi, j=0, k=i).to_sparse()
            else:
                observable = operators.StringCorrelation(hilbert=hi, j=0, k=i).to_sparse()
            #print(observable.shape)
            #print(v.shape)
            #result_l = np.dot(np.dot(v, observable), v).real
            result_l = observable.dot(v).dot(v).real
            #print(result_l, &#39;; &#39;, result_l2)
            results[index] = result_l
    if(dataname == None):
        dataname = &#39;&#39;.join((&#39;L&#39;, str(L), &#39;_exact&#39;))
    dataname = functions.create_path(dataname, path=path)
    dataname = &#39;&#39;.join((dataname, &#39;.csv&#39;))
    # save to csv file
    np.savetxt(dataname, results, delimiter=&#39;;&#39;)
    print(results)
    sys.stdout.flush()
    return results</code></pre>
</details>
</dd>
<dt id="test.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>L=50, alpha=4, sr=None, dataname=None, path='run', machine_name='JaxRBM', sampler='Local', hamiltonian_name='transformed_Heisenberg', n_samples=10000, n_iterations=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to load a pretrained machine and continue the training.</p>
<p>A hamiltonian and sampler can be chosen. The machine is defined and trained for the hamiltonian.</p>
<pre><code>Args:
    L (int) : The number of sites of the lattice
    alpha (int) : A factor to define the size of different machines
    sr (float) : The parameter for stochastic reconfiguration method. If it is None, stochastic reconfiguration is not used
    dataname (str) : The dataname. If None, an automatic dataname is chosen
    path (str) : The directory, where the results are saved. If None, the directory is 'run'
    machine_name (str) A string to choose the machine. Possible inputs: See get_machine in my_machines.py
    sampler (str) : A string to choose the sampler: Recommended: 'Local' (this works with every machine)
    hamiltonian_name (str) : A string to choose the hamiltonian. Possible inputs: see get_hamiltonian in my_models.py
    n_samples (int) : The number of samples used in every iteration step
    n_iterations (int) : The number of iterations (training steps)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(L=__L__, alpha=__alpha__, sr = None, dataname = None, path = &#39;run&#39;, machine_name = &#39;JaxRBM&#39;, sampler = &#39;Local&#39;, hamiltonian_name = &#39;transformed_Heisenberg&#39;, n_samples =10000, n_iterations = 20):
    &#34;&#34;&#34;Method to load a pretrained machine and continue the training.

        A hamiltonian and sampler can be chosen. The machine is defined and trained for the hamiltonian.

            Args:
                L (int) : The number of sites of the lattice
                alpha (int) : A factor to define the size of different machines
                sr (float) : The parameter for stochastic reconfiguration method. If it is None, stochastic reconfiguration is not used
                dataname (str) : The dataname. If None, an automatic dataname is chosen
                path (str) : The directory, where the results are saved. If None, the directory is &#39;run&#39;
                machine_name (str) A string to choose the machine. Possible inputs: See get_machine in my_machines.py
                sampler (str) : A string to choose the sampler: Recommended: &#39;Local&#39; (this works with every machine)
                hamiltonian_name (str) : A string to choose the hamiltonian. Possible inputs: see get_hamiltonian in my_models.py
                n_samples (int) : The number of samples used in every iteration step
                n_iterations (int) : The number of iterations (training steps)

                &#34;&#34;&#34;
    if (dataname == None):
        dataname = &#39;&#39;.join((&#39;L&#39;, str(L)))
    dataname = functions.create_path(dataname, path=path)
    ha, hi, g = models.get_hamiltonian(hamiltonian_name, L)
    print(&#39;uses&#39;, hamiltonian_name, &#39;hamiltonian&#39;)
    sys.stdout.flush()
    print(&#39;load the machine: &#39;, dataname)
    generate_machine = machines.get_machine(machine_name)
    ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
    ma.load(&#39;&#39;.join((dataname, &#39;.wf&#39;)))
    op, sa = machines.load_machine(machine=ma, hamiltonian=ha, optimizer=&#39;Adamax&#39;, lr=0.001, sampler=sampler)
    #observables = functions.get_operator(hilbert=hi, L=L, operator=&#39;FerroCorr&#39;, symmetric=True)
    #observables = {**functions.get_operator(hilbert=hi, L=L, operator=&#39;FerroCorr&#39;, symmetric=False), **functions.get_operator(hilbert=hi, L=L, operator=&#39;FerroCorr&#39;, symmetric=True)}
    if(sr == None):
        gs2 = nk.Vmc(hamiltonian=ha, sampler=sa, optimizer=op, n_samples=n_samples)#, n_discard=5000)
    else:
        sr = nk.optimizer.SR(ma, diag_shift=sr)
        gs2 = nk.Vmc(hamiltonian=ha, sampler=sa, optimizer=op, n_samples=n_samples, sr=sr)#, n_discard=5000)

    functions.create_machinefile(machine_name, L, alpha, dataname, sr)
    start = time.time()
    #gs2.run(n_iter=n_iterations, out=&#39;&#39;.join((dataname, &#39;_load&#39;)), obs=observables, write_every=4, save_params_every=4)
    gs2.run(n_iter=n_iterations, out=dataname, write_every=10, save_params_every=10)
    end = time.time()
    with open(&#39;&#39;.join((dataname, &#39;.time&#39;)), &#39;a&#39;) as reader:
        reader.write(str(end - start))
    #print(gs2.estimate(observables))
    print(&#39;Time&#39;, end - start)
    sys.stdout.flush()</code></pre>
</details>
</dd>
<dt id="test.measureObservable"><code class="name flex">
<span>def <span class="ident">measureObservable</span></span>(<span>L=50, alpha=4, dataname=None, path='run', machine_name='JaxRBM', sampler='Local', hamiltonian_name='transformed_Heisenberg', n_samples=10000, n_iterations=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to measure th observables with a trained machine.</p>
<p>The sampler can be chosen.</p>
<pre><code>Args:
    L (int) : The number of sites of the lattice
    alpha (int) : A factor to define the size of different machines
    dataname (str) : The dataname. If None, an automatic dataname is chosen
    path (str) : The directory, where the results are saved. If None, the directory is 'run'
    machine_name (str) A string to choose the machine. Possible inputs: See get_machine in my_machines.py
    sampler (str) : A string to choose the sampler: Recommended: 'Local' (this works with every machine)
    hamiltonian_name (str) : A string to choose the hamiltonian. Possible inputs: see get_hamiltonian in my_models.py
    n_samples (int) : The number of samples used in every iteration step
    n_iterations (int) : The number of iterations (training steps)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def measureObservable(L=__L__, alpha=__alpha__, dataname = None, path = &#39;run&#39;, machine_name = &#39;JaxRBM&#39;, sampler = &#39;Local&#39;, hamiltonian_name = &#39;transformed_Heisenberg&#39;, n_samples =10000, n_iterations = 20):
    &#34;&#34;&#34;Method to measure th observables with a trained machine.

            The sampler can be chosen.

                Args:
                    L (int) : The number of sites of the lattice
                    alpha (int) : A factor to define the size of different machines
                    dataname (str) : The dataname. If None, an automatic dataname is chosen
                    path (str) : The directory, where the results are saved. If None, the directory is &#39;run&#39;
                    machine_name (str) A string to choose the machine. Possible inputs: See get_machine in my_machines.py
                    sampler (str) : A string to choose the sampler: Recommended: &#39;Local&#39; (this works with every machine)
                    hamiltonian_name (str) : A string to choose the hamiltonian. Possible inputs: see get_hamiltonian in my_models.py
                    n_samples (int) : The number of samples used in every iteration step
                    n_iterations (int) : The number of iterations (training steps)

                    &#34;&#34;&#34;
    if (dataname == None):
        dataname = &#39;&#39;.join((&#39;L&#39;, str(L)))
    dataname = functions.create_path(dataname, path=path)
    ha, hi, g = models.get_hamiltonian(hamiltonian_name, L)
    generate_machine = machines.get_machine(machine_name)
    ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha)
    ma.load(&#39;&#39;.join((dataname, &#39;.wf&#39;)))
    op, sa = machines.load_machine(machine=ma, hamiltonian=ha, optimizer=&#39;Adamax&#39;, lr=0.001, sampler=sampler)
    observables = {**functions.get_operator(hilbert=hi, L=L, operator=&#39;FerroCorr&#39;, symmetric=False),
                   **functions.get_operator(hilbert=hi, L=L, operator=&#39;FerroCorr&#39;, symmetric=True)}
    #save_dict = {}
    for i in range(n_iterations):
        measurement = nk.variational.estimate_expectations(observables, sa, n_samples=n_samples)
        #save_dict[&#39;&#39;.join((&#39;Iteration&#39;, str(i)))] = measurement
        if(i == 0):
            w = csv.writer(open(&#39;&#39;.join((dataname, &#39;_observables&#39;, &#39;.csv&#39;)), &#34;w&#34;))
            for key, val in measurement.items():
                w.writerow([key, val])
        else:
            w = csv.writer(open(&#39;&#39;.join((dataname, &#39;_observables&#39;, &#39;.csv&#39;)), &#34;a&#34;))
            for key, val in measurement.items():
                w.writerow([key, val])
        print(measurement)</code></pre>
</details>
</dd>
<dt id="test.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>L=50, alpha=4, sr=None, dataname=None, path='run', machine_name='JaxRBM', sampler='Local', hamiltonian_name='transformed_Heisenberg', n_samples=700, n_iterations=500)</span>
</code></dt>
<dd>
<div class="desc"><p>Method to train a machine.</p>
<p>A hamiltonian and sampler can be chosen. The machine is defined and trained for the hamiltonian.</p>
<pre><code>Args:
    L (int) : The number of sites of the lattice.
    alpha (int) : A factor to define the size of different machines.
    sr (float) : The parameter for stochastic reconfiguration method. If it is None, stochastic reconfiguration is not used.
    dataname (str) : The dataname. If None, an automatic dataname is chosen
    path (str) : The directory, where the results are saved. If None, the directory is 'run'
    machine_name (str) A string to choose the machine. Possible inputs: See get_machine in my_machines.py
    sampler (str) : A string to choose the sampler: Recommended: 'Local' (this works with every machine)
    hamiltonian_name (str) : A string to choose the hamiltonian. Possible inputs: see get_hamiltonian in my_models.py
    n_samples (int) : The number of samples used in every iteration step
    n_iterations (int) : The number of iterations (training steps)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(L=__L__, alpha=__alpha__, sr = None, dataname = None, path = &#39;run&#39;, machine_name = &#39;JaxRBM&#39;, sampler = &#39;Local&#39;, hamiltonian_name = &#39;transformed_Heisenberg&#39;, n_samples = __number_samples__, n_iterations = __number_iterations__):
    &#34;&#34;&#34;Method to train a machine.

        A hamiltonian and sampler can be chosen. The machine is defined and trained for the hamiltonian.

            Args:
                L (int) : The number of sites of the lattice.
                alpha (int) : A factor to define the size of different machines.
                sr (float) : The parameter for stochastic reconfiguration method. If it is None, stochastic reconfiguration is not used.
                dataname (str) : The dataname. If None, an automatic dataname is chosen
                path (str) : The directory, where the results are saved. If None, the directory is &#39;run&#39;
                machine_name (str) A string to choose the machine. Possible inputs: See get_machine in my_machines.py
                sampler (str) : A string to choose the sampler: Recommended: &#39;Local&#39; (this works with every machine)
                hamiltonian_name (str) : A string to choose the hamiltonian. Possible inputs: see get_hamiltonian in my_models.py
                n_samples (int) : The number of samples used in every iteration step
                n_iterations (int) : The number of iterations (training steps)
                                                    &#34;&#34;&#34;
    ha, hi, g = models.get_hamiltonian(hamiltonian_name, L)
    print(&#39;uses&#39;, hamiltonian_name, &#39;hamiltonian&#39;)
    sys.stdout.flush()
    generate_machine = machines.get_machine(machine_name)
    ma, op, sa, machine_name = generate_machine(hilbert=hi, hamiltonian=ha, alpha=alpha, optimizer=&#39;Adamax&#39;, lr=0.005, sampler=sampler)

    if(sr == None):
        gs = nk.Vmc(hamiltonian=ha, sampler=sa, optimizer=op, n_samples=n_samples)
    else:
        sr = nk.optimizer.SR(ma, diag_shift=sr)
        gs = nk.Vmc(hamiltonian=ha, sampler=sa, optimizer=op, n_samples=n_samples, sr=sr)

    #observables = functions.get_operator(hilbert=hi, L=L, operator=&#39;FerroCorr&#39;)
    if(dataname == None):
        dataname = &#39;&#39;.join((&#39;L&#39;, str(L)))
    dataname = functions.create_path(dataname, path=path)
    print(&#39;&#39;)
    functions.create_machinefile(machine_name, L, alpha, dataname, sr)
    start = time.time()
    gs.run(n_iter=int(n_iterations), out=dataname)#, obs=observables)
    end = time.time()
    with open(&#39;&#39;.join((dataname, &#39;.time&#39;)), &#39;w&#39;) as reader:
        reader.write(str(end - start))
    print(&#39;Time&#39;, end - start)
    sys.stdout.flush()</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="test.exact" href="#test.exact">exact</a></code></li>
<li><code><a title="test.load" href="#test.load">load</a></code></li>
<li><code><a title="test.measureObservable" href="#test.measureObservable">measureObservable</a></code></li>
<li><code><a title="test.run" href="#test.run">run</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>